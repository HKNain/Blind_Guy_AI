# ğŸ‘ï¸â€ğŸ—¨ï¸ BlindVision AI

An AI-powered assistive system for the visually impaired that uses voice input/output to help users **identify objects indoors** and **navigate safely outdoors** using real-time computer vision.

---

## ğŸ” Problem Statement

Visually impaired individuals face significant challenges navigating unfamiliar indoor and outdoor environments. Traditional aids like canes and guide dogs offer limited feedback and awareness. There's a need for an accessible, intelligent system that can:

- Identify objects in indoor spaces.
- Assist in avoiding obstacles outdoors.
- Operate hands-free with voice-based interaction.

**BlindVision AI** bridges this gap with a real-time, voice-controlled assistant that enables greater independence and situational awareness.

---

## ğŸ’¡ Project Overview

BlindVision AI is a **Python-based assistive system** that listens to user commands, processes the environment via camera, and responds with audio feedback. It has two primary modes:

1. **Indoor Mode** ğŸ   
   - Takes voice commands to find and locate specific objects.
   - Uses YOLOv8n object detection for real-time recognition.

2. **Outdoor Mode** ğŸš¶â€â™‚ï¸  
   - Continuously scans for obstacles (like vehicles, etc.).
   - Alerts the user with audio warnings for safe navigation.

---

## ğŸ› ï¸ Tech Stack

| Component | Description |
|----------|-------------|
| ğŸ Python | Core programming language |
| ğŸ§  [Ultralytics YOLOv8n](https://github.com/ultralytics/ultralytics) | Real-time object detection |
| ğŸ™ï¸ Google Speech Recognition | Converts voice commands to text |
| ğŸ”Š Google Text-to-Speech (gTTS) | Converts text responses to speech |
| ğŸ“· OpenCV | Captures and processes video frames |
| ğŸ” PyTorch | Deep learning model runtime |
| ğŸ§ playsound / pyttsx3 | Voice feedback (offline option also available) |

---

## ğŸ¯ Features

- ğŸ¤ **Voice Commands**: Fully voice-operated interface.
- ğŸ§­ **Two Modes**:
  - *Indoor*: Detects requested objects in the environment.
  - *Outdoor*: Alerts user of obstacles using live camera feed.
- ğŸ”ˆ **Audio Feedback**: All responses are spoken aloud.
- ğŸ”Œ **Modular Design**: Easy to switch between modes and extend functionality.
- ğŸ’» **Laptop Prototype**: Runs on standard webcam and microphone.

---

## ğŸ§ª How It Works (Workflow)

1. **Start Application**
2. **Choose Mode via Voice**: "Indoor" or "Outdoor"
3. **Indoor Mode**:
   - User: â€œFind bottleâ€
   - Camera scans environment
   - If found, response: â€œBottle detected at center-leftâ€
4. **Outdoor Mode**:
   - Constant video feed analysis
   - Voice alert: â€œObstacle ahead. Move rightâ€
5. **Repeat until exit command**

---

## ğŸš€ Setup Instructions

1. **Clone this repository**
```bash
git clone https://github.com/HKNain/Blind_Guy_AI.git
cd blindvision-ai
```

## ğŸ§‘â€ğŸ’» Future Scope
- ğŸŒ Migrate to an IoT-based wearable using Raspberry Pi or Jetson Nano

- ğŸ¥½ Build into smart goggles with BLE modules

- ğŸ—ºï¸ Integrate GPS-based path guidance and mapping

## ğŸ’¼ Business Model

- One-time software purchase (Base Tier) : Users pay once to download and install the standard laptop 
version                   
with core features.
### Tiered product upgrades 
- Just like smartphones, users can purchase higher-tier versions (better accuracy, faster processing, improved interface) at a higher one-time price.
- Institutional licensing and bulk access.
- Multi-seat licenses for NGOs, schools, and care homes for centralized management
- Hardware sales (future) 
- Smart wearable goggles with built-in camera, onboard AI, and various models (basic to advanced) available for one-time purchase.
  
## Target Market

- Visually impaired individuals.
- NGOs, accessibility centers, special educationÂ institutes

## ğŸ’™ Impact

- Improves mobility and confidence of visually impaired users

- Promotes independent living

- Reduces dependence on others for daily navigation tasks

- Contributes to inclusive AI for accessibility
